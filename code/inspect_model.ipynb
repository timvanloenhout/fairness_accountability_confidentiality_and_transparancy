{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import copy\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "from torchvision.utils import make_grid\n",
    "print(torch.__version__)\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "print(ROOT_DIR)\n",
    "\n",
    "sys.path.append(ROOT_DIR+\"/DBVAE/\")  # To find local version of the library\n",
    "from db_vae import DBVAE\n",
    "import debias_utils\n",
    "import data_utils\n",
    "import plot_utils\n",
    "import ppb_utils\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare all datasets\n",
    "data_utils.prepare_datasets(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all directories to retrieve models and csv's \n",
    "\n",
    "slash = \"/\"\n",
    "\n",
    "MODELS_DIR_SUM = os.path.join(ROOT_DIR, \"final_results{}sum{}models{}\".format(slash, slash, slash))\n",
    "print(MODELS_DIR_SUM)\n",
    "\n",
    "TRAIN_STATS_DIR_SUM = os.path.join(ROOT_DIR, \"final_results{}sum{}train_stats{}\".format(slash, slash, slash))\n",
    "print(TRAIN_STATS_DIR_SUM)\n",
    "\n",
    "TEST_ACC_DIR_SUM = os.path.join(ROOT_DIR, \"final_results{}sum{}test_accuracy{}\".format(slash, slash, slash))\n",
    "print(TEST_ACC_DIR_SUM)\n",
    "\n",
    "MODELS_DIR_MEAN = os.path.join(ROOT_DIR, \"final_results{}mean{}models{}\".format(slash, slash, slash))\n",
    "print(MODELS_DIR_MEAN)\n",
    "\n",
    "TRAIN_STATS_DIR_MEAN = os.path.join(ROOT_DIR, \"final_results{}mean{}train_stats{}\".format(slash, slash, slash))\n",
    "print(TRAIN_STATS_DIR_MEAN)\n",
    "\n",
    "TEST_ACC_DIR_MEAN = os.path.join(ROOT_DIR, \"final_results{}mean{}test_accuracy{}\".format(slash, slash, slash))\n",
    "print(TEST_ACC_DIR_MEAN)\n",
    "\n",
    "if full_dataset:\n",
    "    TRAIN_DATA_DIR = os.path.join(ROOT_DIR, \"data{}faces{}\".format(slash, slash))\n",
    "else:\n",
    "    TRAIN_DATA_DIR = os.path.join(ROOT_DIR, \"data{}faces_small{}\".format(slash, slash))\n",
    "print(TRAIN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(MODELS_DIR):\n",
    "    models = {} # dictionary with a list of models for every alpha\n",
    "    filenames = {}\n",
    "\n",
    "    for filename in os.listdir(MODELS_DIR):  # iterate over files\n",
    "\n",
    "        alpha = filename.split(\"_\")[2]    \n",
    "\n",
    "        # load the model\n",
    "        model = DBVAE(z_dim=100).to(device)\n",
    "        checkpoint = torch.load(MODELS_DIR + filename , map_location=torch.device(device))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "        \n",
    "        # add model to models dict \n",
    "        if alpha not in models.keys():\n",
    "            models[alpha] = [model]\n",
    "            filenames[alpha] = [filename[:-8]]\n",
    "        else:\n",
    "            models[alpha].append(model)\n",
    "            filenames[alpha].append(filename[:-8])\n",
    "    \n",
    "    return models, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_sum, filenames_sum = load_models(MODELS_DIR_SUM)\n",
    "print(filenames_sum)\n",
    "\n",
    "models_mean, filenames_mean = load_models(MODELS_DIR_MEAN)\n",
    "print(filenames_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_model(models, filenames, alpha=None):\n",
    "    if alpha != None: \n",
    "        key = alpha\n",
    "    else:\n",
    "        keys = list(models.keys())\n",
    "        key = random.choice(keys)\n",
    "\n",
    "    i = random.randint(0, len(models[key]) - 1)\n",
    "    model = models[key][i]\n",
    "    \n",
    "    print(\"Model {} is selected\".format(filenames[key][i]))\n",
    "    \n",
    "    return model, key\n",
    "\n",
    "# -----------------------------------------\n",
    "# or select a model\n",
    "# -----------------------------------------\n",
    "\n",
    "def get_selected_model(model_name, MODELS_DIR):\n",
    "    \n",
    "    alpha = model_name.split(\"_\")[2]    \n",
    "\n",
    "    # load the model\n",
    "    model = DBVAE(z_dim=100).to(device)\n",
    "    checkpoint = torch.load(MODELS_DIR + model_name , map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model {} is selected\".format(model_name))\n",
    "    \n",
    "    return model, alpha        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, alpha = get_random_model(models, filenames, alpha=\"0.001\")\n",
    "model_sum, alpha_sum= get_selected_model(\"model_alpha_0.01_v2.pth.tar\", MODELS_DIR_SUM)\n",
    "\n",
    "model_mean, alpha_mean = get_selected_model(\"model_alpha_0.01_v1.pth.tar\", MODELS_DIR_MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracies(model):\n",
    "    accuracies = []\n",
    "    total_test_accuracy =0\n",
    "    total_num_faces = 0\n",
    "\n",
    "    keys = [\"male_lighter\", \"male_darker\", \"female_lighter\", \"female_darker\"]\n",
    "                    \n",
    "    face_evaluator = ppb_utils.PPBFaceEvaluator()\n",
    "                    \n",
    "    for key in keys:\n",
    "        accuracy, num_faces = face_evaluator.evaluate([model], key, patch_stride=0.2, patch_depth=5)\n",
    "        print(\"Test accuracy for {}: {} \\n\".format(key, round(accuracy[0], 4)))\n",
    "        \n",
    "        total_test_accuracy += (num_faces * round(accuracy[0], 4))\n",
    "        total_num_faces += num_faces\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    total_test_accuracy = total_test_accuracy / total_num_faces\n",
    "    accuracies.append(total_test_accuracy)\n",
    "    print(\"Overall accuracy: {}\".format(total_test_accuracy))\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies = get_test_accuracies(model_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally test each model\n",
    "Each model is already tested after training, with the results saved in final_results/test_accuracy/, therefore this is optional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies_all_models(models, filenames, reduction_type):\n",
    "    groups = [\"male_lighter\", \"male_darker\", \"female_lighter\", \"female_darker\", \"total\"]\n",
    "    \n",
    "    Path(\"final_results/new_test_accuracy/{}/\".format(reduction_type)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    for key in models.keys():\n",
    "        \n",
    "        for i, model in enumerate(models[key]):\n",
    "            \n",
    "            filename = filenames[key][i]\n",
    "            print(\"Model {}\".format(filename))\n",
    "            accuracies = get_test_accuracies(model)\n",
    "            \n",
    "            f= open(\"final_results/new_test_accuracy/{}/{}.csv\".format(reduction_type, filename),\"w+\")\n",
    "            for group, acc in zip(groups, accuracies):\n",
    "                f.write(group + \", \" + str(acc) + \"\\n\") \n",
    "            f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_accuracies_all_models(models_sum, filenames_sum, \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_accuracies_all_models(models_mean, filenames_mean, \"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating image probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities(model, TRAIN_DATA_DIR, alpha, device):\n",
    "    train_data, val_data = data_utils.load_datasets(TRAIN_DATA_DIR)\n",
    "    train_seq_dataloader = torch_data.DataLoader(train_data, batch_size=24)\n",
    "    \n",
    "    latent_means = debias_utils.get_all_latent_means(train_seq_dataloader, model.encoder, 100, device)\n",
    "\n",
    "    sample_probabilities = debias_utils.get_training_sample_probabilities(latent_means,\n",
    "                                                       train_data.labels,\n",
    "                                                       bins=10,\n",
    "                                                       alpha=float(alpha))\n",
    "\n",
    "    return train_data, sample_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, sample_probabilities = get_probabilities(model_sum, TRAIN_DATA_DIR, alpha_sum, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch sampling with and without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_faces(image1, image2, title1, title2):\n",
    "    plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title(title1)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.imshow(image1.permute((1,2,0)))\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(title2)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.imshow(image2.permute((1,2,0)))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_faces(sample_probabilities, train_data):\n",
    "    \n",
    "    train_sampler = torch_data.RandomSampler(train_data)\n",
    "    seq_dataloader = torch_data.DataLoader(train_data,\n",
    "                                                  batch_size=50,\n",
    "                                                  sampler=train_sampler)\n",
    "    \n",
    "    train_sampler = torch_data.WeightedRandomSampler(sample_probabilities, len(sample_probabilities))\n",
    "    prob_dataloader = torch_data.DataLoader(train_data,\n",
    "                                                  batch_size=50,\n",
    "                                                  sampler=train_sampler)\n",
    "    \n",
    "    seq_batch = next(iter(seq_dataloader))\n",
    "    seq_images = seq_batch[0].permute((0,3,1,2))\n",
    "    seq_labels = seq_batch[1].nonzero().squeeze()\n",
    "    seq_images = seq_images[seq_labels]\n",
    "    seq_images = make_grid(seq_images[:12, [2, 1, 0], :, :], nrow=6)\n",
    "\n",
    "    prob_batch = next(iter(prob_dataloader))\n",
    "    prob_images = prob_batch[0].permute((0,3,1,2))\n",
    "    prob_labels = prob_batch[1].nonzero().squeeze()\n",
    "    prob_images = prob_images[prob_labels]\n",
    "\n",
    "    prob_images = make_grid(prob_images[:12, [2, 1, 0], :, :], nrow=6)\n",
    "\n",
    "    \n",
    "    print_faces(seq_images, prob_images, \n",
    "                \"Random batch sampling\", \"Batch sampling with learned debiasing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_faces(sample_probabilities, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faces with highest and lowest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_faces(sample_probabilities, images):\n",
    "    indices_by_prob = sample_probabilities.argsort()\n",
    "    \n",
    "    biggest = indices_by_prob[-12:]\n",
    "    biggest_images = images[biggest].permute((0,3,1,2))\n",
    "    highest = make_grid(biggest_images[:, [2, 1, 0], :, :], nrow=6)\n",
    "\n",
    "    smallest = indices_by_prob[:12]\n",
    "    smallest_images = images[smallest].permute((0,3,1,2))\n",
    "    lowest = make_grid(smallest_images[:, [2, 1, 0], :, :], nrow=6)\n",
    "\n",
    "    print_faces(highest, lowest, \n",
    "                \"Faces with the highest sampling probability.\", \n",
    "                \"Faces with the lowest sampling probability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_faces(sample_probabilities, train_data.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction(model):\n",
    "    \n",
    "    faces = train_data.images[train_data.labels.squeeze().nonzero()].permute(0, 3, 1, 2).float() / 255.\n",
    "    inds = random.choices(list(range(faces.shape[0])), k=16)    \n",
    "    faces =faces[inds]\n",
    "    \n",
    "    mean, logvar, y_pred, reconstruction, z = model(faces)\n",
    "    \n",
    "    faces = make_grid(faces[:, [2, 1, 0], :, :], nrow=int(math.sqrt(16)))\n",
    "    reconstruction = make_grid(reconstruction[:, [2, 1, 0], :, :].detach(), nrow=int(math.sqrt(16)))\n",
    "\n",
    "    print_faces(faces, reconstruction, \n",
    "                \"Original images\", \n",
    "                \"Reconstructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reconstruction(model_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reconstruction(model_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate between images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_interpolate(a, b, i):\n",
    "    mix = b - a\n",
    "    return a + mix * i\n",
    "\n",
    "def interpolate(model):\n",
    "    \n",
    "    plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    \n",
    "    faces = train_data.images[train_data.labels.squeeze().nonzero()].permute(0, 3, 1, 2).float() / 255.\n",
    "    recons = torch.zeros([40, 3, 64, 64])\n",
    "    \n",
    "    numbers = np.linspace(0, 1, num=6)\n",
    "    vector_inter = np.vectorize(element_interpolate)\n",
    "            \n",
    "    for i in range(5):\n",
    "        inds = random.choices(list(range(faces.shape[0])), k=2)\n",
    "        \n",
    "        two_faces =faces[inds]\n",
    "        mean, logvar, y_pred, reconstruction, z = model(two_faces)\n",
    "        \n",
    "        \n",
    "        spaces = []\n",
    "        for num in numbers:\n",
    "            spaces.append(torch.from_numpy(\n",
    "                vector_inter(mean[0].detach(), mean[1].detach(), num)).float().unsqueeze(0))\n",
    "\n",
    "        all_z = torch.cat(spaces, 0)\n",
    "        \n",
    "        reconstructed = model.decoder(all_z)\n",
    "        reconstructed = torch.cat((two_faces[0].unsqueeze(0), reconstructed, two_faces[1].unsqueeze(0))).detach()\n",
    "        recons[i*8:(i+1)*8] = reconstructed[:,[2,1,0],:,:]\n",
    "        \n",
    "    interpolated = make_grid(recons, nrow=8)\n",
    "    plt.imshow(interpolated.permute((1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interpolate(model_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpolate(model_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracies with different alpha's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracies(TEST_ACC_DIR):\n",
    "    \n",
    "    accs = {}\n",
    "    \n",
    "    for filename in os.listdir(TEST_ACC_DIR):  \n",
    "        accuracies = []\n",
    "\n",
    "        alpha = filename.split(\"_\")[2]   \n",
    "        \n",
    "        with open(TEST_ACC_DIR +filename, mode='r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader: \n",
    "                try:\n",
    "                    accuracies.append(float(row[1].strip()[1:-1]))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if alpha not in accs.keys():\n",
    "                accs[alpha] = [accuracies]\n",
    "            else:\n",
    "                accs[alpha].append(accuracies)\n",
    "    \n",
    "    return accs\n",
    "\n",
    "def get_mean_var(list_dict):\n",
    "    \n",
    "    alphas = []\n",
    "    mean = []\n",
    "    var = []\n",
    "\n",
    "    for alpha in list_dict.keys():\n",
    "        alphas.append(alpha)\n",
    "        mean.append(np.array(list_dict[alpha]).mean(axis=0))\n",
    "        var.append(np.array(list_dict[alpha]).var(axis=0))\n",
    "    \n",
    "    return alphas, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accuracy_graph(alphas, mean, var):\n",
    "    \n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "    labels = np.array(['Male Light', 'Male Dark', 'Female Light', 'Female Dark', 'Overall'])\n",
    "    \n",
    "#     alpha_sequence = [\"basic\", \"0.001\", \"0.01\", \"0.05\", \"0.1\", \"nodebias\"] \n",
    "    alpha_sequence = [\"nodebias\",  \"0.1\", \"0.05\", \"0.01\", \"0.001\"] \n",
    "\n",
    "\n",
    "    x = np.arange(len(mean[0]))  # the label locations\n",
    "    width = 0.14  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for i, a in enumerate(alpha_sequence):\n",
    "\n",
    "        idx = alphas.index(alpha_sequence[i])\n",
    "        \n",
    "        rects = ax.bar(x + width*1.04*(i-2.5), mean[idx], width, label=\"\\u03B1 \" + alphas[idx])\n",
    "        plt.errorbar(x + width*1.04*(i-2.5), mean[idx], var[idx], linestyle='None', ecolor='#666666')\n",
    "\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy by skin and gender over different \\u03B1')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    legend = ax.legend(loc='lower left', shadow=True, frameon=True)\n",
    "\n",
    "    # Put a nicer background color on the legend.\n",
    "    frame = legend.get_frame()\n",
    "    frame.set_facecolor('#ffffff')\n",
    "    \n",
    "    ax.set_ylim([0.7,1.0])\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_sum = get_test_accuracies(TEST_ACC_DIR_SUM)\n",
    "\n",
    "alpha_lst, test_mean_lst, test_var_lst = get_mean_var(accuracies_sum)\n",
    "\n",
    "create_accuracy_graph(alpha_lst, test_mean_lst, test_var_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_mean = get_test_accuracies(TEST_ACC_DIR_MEAN)\n",
    "\n",
    "alpha_lst, test_mean_lst, test_var_lst = get_mean_var(accuracies_mean)\n",
    "\n",
    "create_accuracy_graph(alpha_lst, test_mean_lst, test_var_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall and Subgroup variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_sum = get_test_accuracies(TEST_ACC_DIR_SUM)\n",
    "\n",
    "alpha_lst, test_mean_lst, test_var_lst = get_mean_var(accuracies_sum)\n",
    "\n",
    "def accuracy_mean_and_variance(alphas, accuracies):\n",
    "    alphas = np.array(alphas)\n",
    "    idx = np.argwhere(np.array(alphas) != \"basic\").squeeze()\n",
    "    \n",
    "    alphas = alphas[idx]\n",
    "    accuracies = np.array(accuracies)*100\n",
    "    \n",
    "    overall = accuracies[:, -1:].squeeze()[idx].round(2)\n",
    "    subsets = accuracies[:, :-1]\n",
    "    \n",
    "    variance = np.var(subsets, axis=1)[idx].round(2)\n",
    "    \n",
    "    print(\"Alpha:      \\t Recall:      \\t Variance:\")\n",
    "    for i, alpha in enumerate(alphas): \n",
    "        print(\"{}      \\t {}      \\t {}\".format(alpha, overall[i], variance[i]))\n",
    "    \n",
    "    return overall, variance\n",
    "\n",
    "recall, variance = accuracy_mean_and_variance(alpha_lst, test_mean_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loss and validation accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_acc_csv(TRAIN_STATS_DIR):\n",
    "\n",
    "    train_loss = {}\n",
    "    val_acc = {}\n",
    "\n",
    "    for filename in os.listdir(TRAIN_STATS_DIR): \n",
    "\n",
    "        loss = []\n",
    "        acc = []\n",
    "    \n",
    "        alpha = filename.split(\"_\")[2]   \n",
    "        with open(TRAIN_STATS_DIR + filename, mode='r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for i, row in enumerate(reader): \n",
    "                if i != 0: \n",
    "                    loss.append(float(row[0][1:]))\n",
    "                    acc.append(float(row[3][1:]))\n",
    "\n",
    "            if alpha not in train_loss.keys():\n",
    "                train_loss[alpha] = [loss]\n",
    "                val_acc[alpha] = [acc]\n",
    "            else:\n",
    "                train_loss[alpha].append(loss)\n",
    "                val_acc[alpha].append(acc)\n",
    "    \n",
    "    return train_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_loss_val_acc(alpha_lst, train_loss_mean, val_acc_mean):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    ax = plt.subplot(121)\n",
    "    plt.title(\"Average training Loss\")\n",
    "    for i, lst in enumerate(train_loss_mean):\n",
    "        x = list(range(len(lst)))\n",
    "        plt.plot(x, lst, label=\"\\u03B1 \" + alpha_lst[i])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Average validation accuracy\")\n",
    "    for i, lst in enumerate(val_acc_mean):\n",
    "        x = list(range(len(lst)))\n",
    "        plt.plot(x, lst, label=\"\\u03B1 \" + alpha_lst[i])\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_loss, val_acc = get_loss_acc_csv(TRAIN_STATS_DIR_MEAN)\n",
    "# alpha_lst, train_loss_mean, train_loss_var = get_mean_var(train_loss)\n",
    "# alpha_lst, val_acc_mean, val_acc_var = get_mean_var(val_acc)\n",
    "\n",
    "# create_train_loss_val_acc(alpha_lst, train_loss_mean, val_acc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss, val_acc = get_loss_acc_csv(TRAIN_STATS_DIR_SUM)\n",
    "# alpha_lst, train_loss_sum, train_loss_var = get_mean_var(train_loss)\n",
    "# alpha_lst, val_acc_sum, val_acc_var = get_mean_var(val_acc)\n",
    "\n",
    "# create_train_loss_val_acc(alpha_lst, train_loss_sum, val_acc_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot sample probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_histogram(sample_probabilities, labels):\n",
    "\n",
    "    # Show all sample probabilities in a histogram\n",
    "    histogram_density, bin_edges = np.histogram(sample_probabilities[labels.squeeze() == 1], bins=10)\n",
    "    plt.style.use('seaborn')\n",
    "    plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "    plt.hist(bin_edges[:-1], bin_edges, weights=histogram_density, rwidth=0.8, log=True)\n",
    "    plt.ylabel('Number of faces')\n",
    "    plt.xlabel('Probability of resampling')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probabilities_histogram(sample_probabilities, train_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
